# ollama_pi

This is based on running ollama webui locally on raspberry pi 4 model b. 

# ollama Model :- Tinyllama
how to download it :
go to terminal and put this in : curl -fsSL https://ollama.com/install.sh | sh

after it got downloaded, u can check it by typing 'ollama'

next step : u can install the tinyllama model on it

type : 
>ollama run tinyllama


let it download it and it will run automatically and u can chat with it.

Now we gonna work on running the model locally and using it in backend to get and send requests and reponse to users.
